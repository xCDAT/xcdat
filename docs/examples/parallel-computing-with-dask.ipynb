{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Guide for Parallel Computing with Dask\n",
    "\n",
    "Author: [Tom Vo](https://github.com/tomvothecoder)\n",
    "\n",
    "Last Updated: 11/07/24 (v0.7.3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook serves as a general guide for parallel computing with xCDAT. It covers the\n",
    "following topics:\n",
    "\n",
    "- Dask Best Practices\n",
    "- Xarray and Dask\n",
    "- An Overview of Chunking in Dask\n",
    "- Using a Dask Cluster for Scalable Computations\n",
    "- Code Example - Parallelizing xCDAT Computations with Dask (Local Machine/Login Node)\n",
    "- Code Example - Parallelizing xCDAT Computations with Dask (HPC/Compute Node)\n",
    "- More Resources\n",
    "- FAQs\n",
    "\n",
    "_The data used in this example can be found in the [xarray-data repository](https://github.com/pydata/xarray-data)._\n",
    "\n",
    "Users can [install their own instance of xcdat](../getting-started-guide/installation.rst) and follow these examples using their own environment (e.g., with vscode, Jupyter, Spyder, iPython) or [enable xcdat with existing JupyterHub instances](../getting-started-guide/getting-started-hpc-jupyter.rst). The conda environment used in this notebook includes xcdat, xesmf, matplotlib, ipython, ipykernel, cartopy, and jupyter:\n",
    "\n",
    "```bash\n",
    "conda create -n xcdat_notebook_dask -c conda-forge xcdat matplotlib ipython ipykernel cartopy nc-time-axis jupyter jupyter-server-proxy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source\n",
    "\n",
    "This notebook excerpts a information about Dask from other documentation pages and\n",
    "summarizes the general concepts and principles.\n",
    "\n",
    "Main pages include:\n",
    "\n",
    "- https://docs.dask.org/en/stable/array-best-practices.html#best-practices\n",
    "- https://docs.xarray.dev/en/stable/user-guide/dask.html\n",
    "- https://docs.dask.org/en/latest/array-chunks.html\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Best Practices\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../_static/dask-logo.svg\" alt=\"Dask logo\" style=\"display: inline-block; width:300px;\">\n",
    "</div>\n",
    "\n",
    "- **Use NumPy**\n",
    "  - If your data fits comfortably in RAM and you are not performance bound, then using NumPy might be the right choice.\n",
    "  - Dask adds another layer of complexity which may get in the way.\n",
    "  - If you are just looking for speedups rather than scalability then you may want to consider a project like [Numba](https://numba.pydata.org/)\n",
    "- **Select a good chunk size**\n",
    "  - A common performance problem among Dask Array users is that they have chosen a chunk size that is either too small (leading to lots of overhead) or poorly aligned with their data (leading to inefficient reading).\n",
    "- **Orient your chunks**\n",
    "  - When reading data you should align your chunks with your storage format.\n",
    "- **Avoid Oversubscribing Threads**\n",
    "  - By default Dask will run as many concurrent tasks as you have logical cores. It assumes that each task will consume about one core. However, many array-computing libraries are themselves multi-threaded, which can cause contention and low performance.\n",
    "- **Consider Xarray**\n",
    "  - The Xarray package wraps around Dask Array, and so offers the same scalability, but also adds convenience when dealing with complex datasets\n",
    "- **Build your own Operations**\n",
    "  - Often we want to perform computations for which there is no exact function in Dask Array. In these cases we may be able to use some of the more generic functions to build our own.\n",
    "\n",
    "&mdash; <cite>https://docs.dask.org/en/stable/array-best-practices.html#best-practices</cite>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basics of Dask Arrays\n",
    "\n",
    "- **Dask divides arrays** into many small pieces, called **\"chunks\"** (each presumed to be small enough to fit into memory)\n",
    "- Dask Array **operations are lazy**\n",
    "  - Operations **queue** up a series of tasks mapped over blocks\n",
    "  - No computation is performed until values need to be computed (hence \"lazy\")\n",
    "  - Data is loaded into memory and **computation** is performed in **streaming fashion**, **block-by-block**\n",
    "- Computation is controlled by multi-processing or thread pool\n",
    "\n",
    "&mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html</cite>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../_static/dask-array.png\" alt=\"Dask Array\" style=\"display: inline-block; width:300px;\">\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xarray and Dask\n",
    "\n",
    "<div style=\"text-align: center\">\n",
    "    <img src=\"../_static/xarray-logo.png\" alt=\"xarray logo\" style=\"display: inline-block; margin-right: 50px; width:400px;\">\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does Xarray integrate with Dask?**\n",
    "\n",
    "> Xarray integrates with Dask to support parallel computations and streaming computation\n",
    "> on datasets that don’t fit into memory. Currently, Dask is an entirely optional feature\n",
    "> for xarray. However, the benefits of using Dask are sufficiently strong that Dask may\n",
    "> become a required dependency in a future version of xarray.\n",
    "\n",
    "&mdash; <cite>https://docs.xarray.dev/en/stable/use\n",
    "\n",
    "**Which Xarray features support Dask?**\n",
    "\n",
    "> Nearly all existing xarray methods (including those for indexing, computation,\n",
    "> concatenating and grouped operations) have been extended to work automatically with\n",
    "> Dask arrays. When you load data as a Dask array in an xarray data structure, almost\n",
    "> all xarray operations will keep it as a Dask array; when this is not possible, they\n",
    "> will raise an exception rather than unexpectedly loading data into memory.\n",
    "\n",
    "&mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html#using-dask-with-xarray</cite>\n",
    "\n",
    "**What is the default Dask behavior for distributing work on compute hardware?**\n",
    "\n",
    "> By default, dask uses its multi-threaded scheduler, which distributes work across\n",
    "> multiple cores and allows for processing some datasets that do not fit into memory.\n",
    "> For running across a cluster, [setup the distributed scheduler](https://docs.dask.org/en/latest/setup.html).\n",
    "\n",
    "&mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html#using-dask-with-xarray</cite>\n",
    "\n",
    "**How do I use Dask arrays in an `xarray.Dataset`?**\n",
    "\n",
    "> The usual way to create a Dataset filled with Dask arrays is to load the data from a\n",
    "> netCDF file or files. You can do this by supplying a `chunks` argument to [open_dataset()](https://docs.xarray.dev/en/stable/generated/xarray.open_dataset.html#xarray.open_dataset)\n",
    "> or using the [open_mfdataset()](https://docs.xarray.dev/en/stable/generated/xarray.open_mfdataset.html#xarray.open_mfdataset) function.\n",
    "\n",
    "**What happens if I don't specify `chunks` with `open_mfdataset()`?**\n",
    "\n",
    "> `open_mfdataset()` called without `chunks` argument will return dask arrays with\n",
    "> chunk sizes equal to the individual files. Re-chunking the dataset after creation\n",
    "> with `ds.chunk()` will lead to an ineffective use of memory and is not recommended.\n",
    "\n",
    "&mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html#reading-and-writing-data</cite>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Overview of Chunking in Dask\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For performance, a good choice of `chunks` follows the following rules:\n",
    ">\n",
    "> 1. A chunk should be small enough to fit comfortably in memory. We'll\n",
    ">    have many chunks in memory at once\n",
    "> 2. A chunk must be large enough so that computations on that chunk take\n",
    ">    significantly longer than the 1ms overhead per task that Dask scheduling\n",
    ">    incurs. A task should take longer than 100ms\n",
    "> 3. Chunk sizes between 10MB-1GB are common, depending on the availability of\n",
    ">    RAM and the duration of computations\n",
    "> 4. Chunks should align with the computation that you want to do.\n",
    ">    - For example, if you plan to frequently slice along a particular dimension,\n",
    ">      then it's more efficient if your chunks are aligned so that you have to\n",
    ">      touch fewer chunks. If you want to add two arrays, then its convenient if\n",
    ">      those arrays have matching chunks patterns\n",
    "> 5. Chunks should align with your storage, if applicable.\n",
    ">    - Array data formats are often chunked as well. When loading or saving data,\n",
    ">      if is useful to have Dask array chunks that are aligned with the chunking\n",
    ">      of your storage, often an even multiple times larger in each direction\n",
    "\n",
    "&mdash; <cite>https://docs.dask.org/en/latest/array-chunks.html</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good rule of thumb with chunking\n",
    "\n",
    "- **Create arrays with a minimum chunksize of at least one million elements (e.g., a 1000x1000 > matrix).**\n",
    "\n",
    "- **With large arrays (10+ GB)**, the cost of queueing up Dask operations can be noticeable and **you may need even > larger chunksizes**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatively, you can let Dask automatically chunk for you then **optimize after**\n",
    "\n",
    "- Dask Arrays can look for a `.chunks` attribute and use that to provide baseline\n",
    "  chunking. This can help prevent users from specifying \"too many chunks\" and\n",
    "  \"too few chunks\" which can lead to performance issues.\n",
    "\n",
    "- Automatic chunking expands or contracts all dimensions marked with `\"auto\"` to try to\n",
    "  reach chunk sizes with a number of bytes equal to the config value `array.chunk-size`,\n",
    "  which is set to **128MiB by default**, but which you can change in your configuration.\n",
    "\n",
    "- <div class=\"alert alert-block alert-info\"><b>Notice:</b> Although Dask's chunk auto-scaling tries its best to optimally align chunks to the ideal sizes using `array.chunks-size`, the auto-scaling is not optimal for ALL use cases. <b>It is still recommended to manually chunk for ideal sizes once you are comfortable doing so</b>.</div>\n",
    "\n",
    "&mdash; <cite>https://docs.dask.org/en/latest/array-chunks.html#automatic-chunking</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking with Xarray and xCDAT using the `chunks` parameter\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this in `open_dataset()` and `open_mfdataset()`, you need to specify the\n",
    "`chunks` parameter either by:\n",
    "\n",
    "1. `chunks={\"time\": \"10\"}` - chunk the specified dimension(s) by a specified number integer\n",
    "2. `chunks={\"time\": \"auto\"}` - auto-scale the specified dimension(s) to get to accommodate ideal chunk sizes. In this example, replace `\"time\"` and/or add additional dims to the dictionary for additional auto-scaling.\n",
    "3. `chunks=\"auto\"` - allow chunking _all_ dimensions to accommodate ideal chunk sizes\n",
    "\n",
    "&mdash; <cite>https://docs.xarray.dev/en/stable/user-guide/dask.html#chunking-and-performance</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Dask Cluster for Scalable Computations\n",
    "\n",
    "> 1. All of the large-scale Dask collections like Dask Array, Dask DataFrame, and Dask Bag and the fine-grained APIs like delayed and futures **generate task graphs** where each node in the graph is a normal Python function and edges between nodes are normal Python objects that are created by one task as outputs and used as inputs in another task.\n",
    ">\n",
    "> 2. After Dask generates these task graphs, it needs to execute them on parallel hardware. This is the job of a **task scheduler**.\n",
    ">\n",
    "> 3. Different task schedulers exist, and each will consume a task graph and compute the same result, but with different performance characteristics. Dask has two families of task schedulers:\n",
    ">\n",
    ">    - **Single-machine scheduler**: This scheduler provides basic features on a local process or thread pool. This scheduler was made first and is the default. It is simple and cheap to use, although it can only be used on a single machine and does not scale\n",
    ">    - **Distributed scheduler**: This scheduler is more sophisticated, offers more features, but also requires a bit more effort to set up. It can run locally or distributed across a cluster\n",
    "\n",
    "&mdash; <cite>https://docs.dask.org/en/stable/scheduling.html</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../_static/dask-overview-schedulers.svg\" alt=\"Dask Schedulers\" style=\"display: inline-block;\">\n",
    "</div>\n",
    "\n",
    "&mdash; <cite>https://docs.dask.org/en/stable/scheduling.html#dask-distributed-local</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Example - Parallelizing xCDAT Computations with Dask (Local Machine/Login Node)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Notice:</b> This section sets up a local cluster directly on whatever machine you're working on. If you are on a login node, it will setup the cluster on there. The login node might suffice for smallers jobs requiring less compute power. For larger jobs on HPC environments that require more compute power, make sure to look at <strong >Code Example - Parallelizing xCDAT Computations with Dask (HPC/Compute Node)</strong>.</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"><b>Disclaimer:</b> The dataset used in the example is only a few hundred MBs to make downloading the input fast. This notebook demonstrates how to\n",
    "get up and running with Dask quickly, and does not aim to show the real-world performance improvements on large datasets.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import xcdat as xc\n",
    "\n",
    "# Silence flox logger info messages.\n",
    "logger = logging.getLogger(\"flox\")\n",
    "logger.setLevel(logging.WARNING)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup the Dask Cluster\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><b>Note:</b> You can skip \"1. Setup the Dask Cluster\"\n",
    "and \"2. Open the Dask Dashboard UI\" if you want to use Xarray with Dask's default multi-threaded scheduler with a chunking configuration of 128 MiB per chunk. <strong>However, it is highly recommended that you set up the Dask cluster with the instructions below to enable more precise Dask configuration based on your machine specifications and resource requirements.</strong> </div>\n",
    "\n",
    "We will quickly setup a local cluster using the Dask `Client` and `LocalCluster`\n",
    "Python modules.\n",
    "\n",
    "You can configure the Dask Client (e.g., memory limit) to your needs. In this case,\n",
    "we are deploying a cluster with:\n",
    "\n",
    "- `n_workers=2`: 2 workers\n",
    "- `threads_per_worker=1`: 1 thread per worker, since we're using processes instead of threads\n",
    "- `memory_limit=\"4GB\"`: 4 GB memory limit per worker, dependent on the availability memory in your system. If the `memory_limit` given is greater than the available memory, the total available memory will be set for each worker\n",
    "- `processes=True`: use processes instead of threads (preferred for most Python code)\n",
    "\n",
    "For info on cluster configurations, visit these links:\n",
    "\n",
    "- https://distributed.dask.org/en/latest/api.html#client\n",
    "- https://distributed.dask.org/en/latest/api.html#distributed.LocalCluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "cluster = LocalCluster(\n",
    "    n_workers=2, threads_per_worker=1, memory_limit=\"4G\", processes=True\n",
    ")\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Open the Dask Dashboard UI\n",
    "\n",
    "The Dask distributed scheduler provides an interactive dashboard containing many plots\n",
    "and tables with live information.\n",
    "\n",
    "Check this [Dask documentation page](https://docs.dask.org/en/stable/dashboard.html) to learn how to interpret the information. There is also a general guide later down in the notebook.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "  <img src=\"../_static/dask-dashboard-example.png\" alt=\"Dask Dashboard UI Example\" style=\"display: inline-block; width:800px;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open the link to the dashboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.dashboard_link"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Open a dataset with xCDAT and chunk it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xc.tutorial.open_dataset(\"ersstv5\", chunks=\"auto\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the data variables contain `dask.array`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Run your computations while viewing the dashboards in your browser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bad signs to watch out for in the dashboard\n",
    "\n",
    "Taskstream plot:\n",
    "\n",
    "- Lots of **white space** in the task stream plot\n",
    "  - Nothing is happening.\n",
    "  - **Chunks may be too small.**\n",
    "- Lots and lots of <span style=\"color:red\">red </span> in the task stream plot\n",
    "  - Represents worker communication.\n",
    "  - Dask workers need some communication, but if they are doing almost nothing except communication then there is not much productive work going on.\n",
    "\n",
    "Worker memory plot:\n",
    "\n",
    "- <span style=\"color:orange\">Orange bars </span> bars which are a sign you are getting close to the memory limit.\n",
    "  - **Chunks may be too big.**\n",
    "- <span style=\"color:gray\">Gray bars </span> which mean data is being spilled to disk.\n",
    "  - **Chunks may be too big.**\n",
    "\n",
    "&mdash; <cite>https://blog.dask.org/2021/11/02/choosing-dask-chunk-sizes</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we queue up the lazy `group_average` operation in Dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_avg = ds.temporal.group_average(\"sst\", freq=\"month\")\n",
    "\n",
    "sst_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can trigger the queued up operations in the Dask task graph using `.compute()` or `.load()`\n",
    "\n",
    "- [.compute()](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.compute.html#xarray-dataset-compute) - Manually trigger loading and/or computation of this dataset’s data from disk or a remote source into memory and return a new dataset. Unlike load, the original dataset is left unaltered.\n",
    "- [.load()](https://docs.xarray.dev/en/stable/generated/xarray.Dataset.load.html) - Manually trigger loading and/or computation of this dataset’s data from disk or a remote source into memory and return this dataset. Unlike compute, the original dataset is modified and returned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You might see `UserWarning: Sending large graph of size 420.59 MiB.`\n",
    "# which indicates that the Dask configuration might not be optimal for the\n",
    "# small dataset used in the example.\n",
    "sst_avg_res = sst_avg.compute()\n",
    "# Or call sst_avg.load() to modify the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explicit close the Dask Client\n",
    "\n",
    "It will close automatically when closing the notebook or killing the Python session too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it! You just performed parallel computing using Dask on your local machine!\n",
    "\n",
    "If you are on an HPC environment where interactive jobs must be requested on compute nodes\n",
    "for larger scale computations, please follow the guide below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Example - Parallelizing xCDAT Computations with Dask (HPC/Compute Node)\n",
    "\n",
    "On an HPC environment, you typically need to request a interactive job on a compute node(s)\n",
    "for your computational needs. The guide below shows you how to do just that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Start an interactive job\n",
    "\n",
    "The first step if you're an HPC environment is to start a Dask job to get nodes.\n",
    "\n",
    "Depending on your HPC environment, you might need to run `salloc`, `srun`, etc. to\n",
    "start an interactive job. Please refer to the documentation for your HPC environment.\n",
    "Here's the documentation from NERSC for [Interactive Jobs](https://docs.nersc.gov/jobs/interactive/).\n",
    "\n",
    "In the example below on the NERSC Perlmutter machine, we're requesting 1 CPU node from\n",
    "the reservation and divide the resources on that node among 10 Dask workers. If you're\n",
    "working on a reservation, request the respective number of hours needed for\n",
    "your node (e.g., 4 hrs/240 mins). When you are working outside a reservation, its best\n",
    "to limit your requested time to your expected working window.\n",
    "\n",
    "`salloc --reservation=dask_day1 -C cpu -N 1 -n 10 -A ntrain5 -t 240`\n",
    "\n",
    "It may take a moment to start, but once it does, you'll get your prompt back.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processes or Threads?\n",
    "\n",
    "Python has the [global interpreter lock (GIL)](https://wiki.python.org/moin/GlobalInterpreterLock),\n",
    "which basically means that Python does not leverage multiple threads well.\n",
    "\n",
    "The general exceptions to this rule include code that is mainly I/O (e.g., downloading\n",
    "data) or code that leverages mostly C++ and other non-Python libraries (e.g., NumPy).\n",
    "\n",
    "For most use cases, using processes over threads might make more sense. Here's an example\n",
    "with `dask worker`:\n",
    "\n",
    "- I have 20 GB of RAM\n",
    "- I have 2 cores. So I want 2 workers.\n",
    "  - This means each worker can consume 10 GB of RAM.\n",
    "\n",
    "```bash\n",
    "$ dask worker tcp://127.0.0.1:8786 --nworkers 2 --nthreads 1 --memory-limit 20GB\n",
    "```\n",
    "\n",
    "&mdash; <cite>https://saturncloud.io/blog/local-cluster/</cite>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Start the Dask Cluster\n",
    "\n",
    "There are several ways to deploy a Dask Cluster. You can check them out [here](https://docs.dask.org/en/stable/deploying.html). For this section, we'll be manually deploying a cluster in your HPC environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a terminal and start the Dask scheduler:\n",
    "\n",
    "```bash\n",
    "$ conda activate xcdat_notebook_dask\n",
    "$ dask-scheduler\n",
    "```\n",
    "\n",
    "The default address for the `dask-scheduler` is `tcp://127.0.0.1:8786`.\n",
    "\n",
    "Open a second terminal and start a Dask worker:\n",
    "\n",
    "```bash\n",
    "$ conda activate xcdat_notebook_dask\n",
    "$ dask worker tcp://127.0.0.1:8786 --nworkers 2 --nthreads=1 --memory-limit 20GB\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Connect the Dask Client to the Cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of this notebook, ignore the VersionMismatchWarning if you get\n",
    "# one.\n",
    "# Related issue: https://github.com/dask/distributed/issues/3767\n",
    "client_hpc = Client(\"tcp://127.0.0.1:8786\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see this output in your `dask-scheduler` terminal:\n",
    "\n",
    "```bash\n",
    "2024-04-30 10:24:42,155 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47522\n",
    "2024-04-30 10:24:44,609 - distributed.scheduler - INFO - Receive client connection: Client-894d078d-0716-11ef-9ffd-f4e9d4af2192\n",
    "2024-04-30 10:24:44,610 - distributed.core - INFO - Starting established connection to tcp://127.0.0.1:47748\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_hpc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perform Computations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_hpc = xc.open_dataset(filepath, chunks=\"auto\")\n",
    "\n",
    "sst_avg_hpc = ds.temporal.group_average(\"sst\", freq=\"month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_avg_hpc_res = sst_avg_hpc.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: You might see these warnings:\n",
    "\n",
    "From the code cell above:\n",
    "\n",
    "```python\n",
    "UserWarning: Sending large graph of size 426.18 MiB.\n",
    "This may cause some slowdown.\n",
    "Consider scattering data ahead of time and using futures.\n",
    "```\n",
    "\n",
    "In the `dask worker` terminal:\n",
    "\n",
    "```bash\n",
    "2024-04-30 10:26:04,806 - distributed.core - INFO - Event loop was unresponsive in Worker for 8.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.\n",
    "```\n",
    "\n",
    "These warnings indicate sub-optimal Dask configurations based on the available resources,\n",
    "non-appropriate usage of Dask for parallelizing (e.g., very small files) , etc. As noted\n",
    "earlier, we are using a small dataset in the example for ease of download, which is not\n",
    "necessarily the best use of Dask.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Resources\n",
    "\n",
    "To learn more in-depth about Dask and Xarray, please check these resources out:\n",
    "\n",
    "- [Official Xarray Parallel Computing with Dask Guide](https://docs.xarray.dev/en/stable/user-guide/dask.html)\n",
    "- [Official Xarray Parallel Computing with Dask Jupyter Notebook Tutorial](https://tutorial.xarray.dev/intermediate/xarray_and_dask.html)\n",
    "- [Official Dask guide for Xarray with Dask Arrays](https://examples.dask.org/xarray.html)\n",
    "- [Project Pythia: Dask Arrays with Xarray](https://foundations.projectpythia.org/core/xarray/dask-arrays-xarray.html)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAQs\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there any other optimizations tips for working with Dask and Xarray?\n",
    "\n",
    "We HIGHLY recommend checking out the [Optimization Tips](https://docs.xarray.dev/en/stable/user-guide/dask.html#optimization-tips) section if you are using Dask with Xarray\n",
    "\n",
    "### Are there cases where xCDAT loads Dask arrays into memory?\n",
    "\n",
    "As of `xarray=2023.5.0`, Xarray does not support updating/setting multi-dimensional dask\n",
    "arrays. The following error is raised if this is attempted: `xarray can't set arrays with multiple array indices to dask yet`.\n",
    "\n",
    "As a workaround, xCDAT loads coordinate bounds into memory if they are multi-dimensional\n",
    "Dask arrays before performing operations or computations. This loading occurs in the\n",
    "following APIs:\n",
    "\n",
    "- `xcdat.axis.swap_lon_axis`\n",
    "  - swapping longitude axis orientation\n",
    "  - aligning longitude bounds to (0, 360) axis\n",
    "- `xarray.Dataset.spatial.average`\n",
    "  - generating weights using lat/lon coordinate bounds\n",
    "  - swapping longitude axis orientation\n",
    "  - scaling domain bounds to a specified region\n",
    "- `xcdat.Dataset.temporal.<average|group_average|climatology|departures>`\n",
    "  - generating weights using time coordinate bounds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xcdat_notebook_0.7.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
